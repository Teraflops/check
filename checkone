import org.apache.spark.sql.{DataFrame, Row, SparkSession}
import org.apache.spark.sql.types._
import org.apache.spark.sql.functions._
import org.apache.spark.Partitioner

// ---- helpers ----
@inline private def s(x: Any): String = if (x == null) "" else x.toString

object Routing extends Serializable {
  val outSchema: StructType = StructType(Seq(
    StructField("_META_ID", StringType, true),
    StructField("fileType", StringType, true),
    StructField("payload",  StringType, true),
    StructField("status",   StringType, true)
  ))

  val csvRow: Row => Row = r => {
    val id  = s(r.getAs[Any]("_META_ID"))
    val ft  = s(r.getAs[Any]("fileType"))
    val raw = s(r.getAs[Any]("raw"))
    Row(id, ft, raw.trim, "CSV_OK")
  }

  val jsonRow: Row => Row = r => {
    val id  = s(r.getAs[Any]("_META_ID"))
    val ft  = s(r.getAs[Any]("fileType"))
    val raw = s(r.getAs[Any]("raw"))
    Row(id, ft, raw.replaceAll("\\s+",""), "JSON_OK")
  }

  val xmlRow: Row => Row = r => {
    val id  = s(r.getAs[Any]("_META_ID"))
    val ft  = s(r.getAs[Any]("fileType"))
    val raw = s(r.getAs[Any]("raw"))
    Row(id, ft, raw, "XML_OK")
  }

  val defRow: Row => Row = r => {
    val id  = s(r.getAs[Any]("_META_ID"))
    val ft  = s(r.getAs[Any]("fileType"))
    val raw = s(r.getAs[Any]("raw"))
    Row(id, ft, raw, "UNSUPPORTED")
  }
}

final class FileTypePartitioner(typeToPid: Map[String, Int]) extends Partitioner with Serializable {
  override def numPartitions: Int = math.max(1, typeToPid.size)
  override def getPartition(key: Any): Int = {
    val k = s(key)
    typeToPid.getOrElse(if (k.isEmpty) "NULL_FT" else k, 0)
  }
}

def processByFileType(dfIn: DataFrame)(implicit spark: SparkSession): DataFrame = {
  import spark.implicits._

  // 0) assert required columns exist
  val need = Set("_META_ID","fileType","raw")
  val have = dfIn.columns.toSet
  require(need.subsetOf(have), s"Missing columns: ${(need -- have).mkString(",")}")

  // 1) sanitize nulls and types
  val withType = dfIn
    .withColumn("_META_ID", col("_META_ID").cast("string"))
    .withColumn("fileType", coalesce(col("fileType").cast("string"), lit("NULL_FT")))
    .withColumn("raw",      col("raw").cast("string"))

  // 2) driver: partition map (NULL_FT included)
  val types     = withType.select("fileType").distinct.as[String].collect.map(x => if (x==null || x.isEmpty) "NULL_FT" else x)
  val typeToPid = types.distinct.zipWithIndex.toMap

  // 3) executors: keyed and partitioned
  val keyed  = withType.rdd.map(r => (s(r.getAs[Any]("fileType")) match {
    case "" => "NULL_FT"; case x => x
  }, r))
  val byType = keyed.partitionBy(new FileTypePartitioner(typeToPid))

  // 4) mapPartitions with full null safety
  val outRdd = byType.mapPartitions { it =>
    if (!it.hasNext) Iterator.empty
    else {
      val buf    = it.buffered
      val ftype0 = buf.head._1
      val ftype  = if (ftype0 == null || ftype0.isEmpty) "NULL_FT" else ftype0
      val fn: Row => Row = ftype match {
        case "CSV"  => Routing.csvRow
        case "JSON" => Routing.jsonRow
        case "XML"  => Routing.xmlRow
        case _      => Routing.defRow
      }
      buf.map { case (_, r) => fn(r) }
    }
  }

  spark.createDataFrame(outRdd, Routing.outSchema)
}
