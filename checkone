// ---------- driver-safe, serializable routing ----------
object Routing extends Serializable {
  type RowIter = Iterator[Row]
  val outSchema = StructType(Seq(
    StructField("_META_ID", StringType, true),
    StructField("fileType", StringType, true),
    StructField("payload",  StringType, true),
    StructField("status",   StringType, true)
  ))

  // Row -> Row transformers (each is Serializable)
  val csvRow: Row => Row = r =>
    Row(r.getAs[String]("_META_ID"), r.getAs[String]("fileType"),
        Option(r.getAs[String]("raw")).map(_.trim).getOrElse(""), "CSV_OK")

  val jsonRow: Row => Row = r =>
    Row(r.getAs[String]("_META_ID"), r.getAs[String]("fileType"),
        Option(r.getAs[String]("raw")).map(_.replaceAll("\\s+","")).getOrElse(""), "JSON_OK")

  val xmlRow: Row => Row = r =>
    Row(r.getAs[String]("_META_ID"), r.getAs[String]("fileType"),
        Option(r.getAs[String]("raw")).getOrElse(""), "XML_OK")

  val defaultRow: Row => Row = r =>
    Row(r.getAs[String]("_META_ID"), r.getAs[String]("fileType"),
        Option(r.getAs[String]("raw")).getOrElse(""), "UNSUPPORTED")

  // Per-partition runner. No Spark refs inside.
  val runPartition: Iterator[(String, Row)] => Iterator[Row] = it => {
    if (!it.hasNext) Iterator.empty
    else {
      val buf   = it.buffered
      val ftype = buf.head._1
      val rowFn = ftype match {
        case "CSV"  => csvRow
        case "JSON" => jsonRow
        case "XML"  => xmlRow
        case _      => defaultRow
      }
      buf.map { case (_, r) => rowFn(r) }
    }
  }
}

// Serializable custom partitioner defined at top level
final class FileTypePartitioner(typeToPid: Map[String, Int]) extends Partitioner with Serializable {
  override def numPartitions: Int = math.max(1, typeToPid.size)
  override def getPartition(key: Any): Int =
    typeToPid.getOrElse(Option(key).map(_.toString).getOrElse("NULL_FT"), 0)
}




writeStream.foreachBatch { (df: DataFrame, epochId: Long) =>
  val withType = df.withColumn("fileType", coalesce(col("fileType").cast("string"), lit("NULL_FT")))

  // driver work only
  val types      = withType.select("fileType").distinct.as[String].collect
  val typeToPid  = types.zipWithIndex.toMap

  // executors get only (String, Row) + serializable partitioner + serializable function
  val keyed   = withType.rdd.map(r => (r.getAs[String]("fileType"), r))
  val byType  = keyed.partitionBy(new FileTypePartitioner(typeToPid))
  val outRdd  = byType.mapPartitions(Routing.runPartition)

  // back to DF on driver
  val outDf = spark.createDataFrame(outRdd, Routing.outSchema)
  outDf.show(false) // or write to Couchbase here via a *new* writer created in this scope
}.start()





