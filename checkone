version: v1

tasks:
  - ref: java-maven-build
    name: java11-build
    params:
      - name: jdk-version
        value: "11"
      - name: context-dir
        value: "./AdditionalTransform"
      - name: goals
        value:
          - clean
          - package

  - ref: java-maven-build
    name: java17-build
    params:
      - name: jdk-version
        value: "17"
      - name: context-dir
        value: "./AdditionalTransform"
      - name: goals
        value:
          - clean
          - package

  - ref: ansible-distribution-publish
    name: jar-publish
    params:
      - name: source-file-path
        value: "./AdditionalTransform/target/nextgen-citidirect-3.0.1.jar"   # adjust path
      - name: artifact-name
        value: "nextgen-citidirect-3.0.1.jar"
    runAfter:
      - java11-build

  - ref: ansible-distribution-publish
    name: jar17-publish
    params:
      - name: source-file-path
        value: "./AnotherTransform/target/nextgen-citidirect-3.0.1.jar"      # adjust path
      - name: artifact-name
        value: "nextgen-citidirect-3.0.1.jar"
    runAfter:
      - java17-build

  - ref: ansible-distribution-publish
    name: publish-config
    params:
      - name: source-file-path
        value: "./config"
      - name: artifact-name
        value: "config"
    runAfter:
      - java17-build

  - ref: ansible-distribution-publish
    name: publish-





import org.apache.spark.sql.SparkSession

val mongoUser = "CTISDQMDB"
val mongoPass = "<password-from-cyberark-or-env>"

val mongoHosts =
  "maas-gt-p338-l2001.nam.nsroot.net:37017," +
  "maas-mw-p319-l2001.nam.nsroot.net:37017," +
  "maas-mw-p259-l2002.nam.nsroot.net:37017"

val database   = "Catalog"
val collection = "DQ_RT_STATUS"

val additionalParams = "authSource=admin&readPreference=primary&ssl=true"

val mongoUri =
  s"mongodb://$mongoUser:$mongoPass@$mongoHosts/$database?$additionalParams"

val spark = SparkSession.builder()
  .appName("ReadFromMongoDQ")
  .config("spark.mongodb.read.connection.uri", mongoUri)
  .config("spark.mongodb.write.connection.uri", mongoUri)
  .getOrCreate()

val df = spark.read
  .format("mongodb")
  .option("database", database)
  .option("collection", collection)
  .load()

df.show(false)

val selected = df.select("RUN_GROUP", "COLUMN_NAME", "CHECK_TYPE", "microbatch_id")
selected.show(false)


